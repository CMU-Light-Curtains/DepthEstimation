\section{Prior Work}

\textbf{Depth from Active Sensors: } Active sensors use a static or fixed scan light source / receiver to perceive depth. Long range outdoor depth from these such as commercially available Time-Of-Flight cameras \cite{quanergy} \cite{luminar} or LIDARs \cite{velodyne} \cite{ouster} provide dense metric depth with confidence values with wide usage in research \cite{Geiger2013IJRR} \cite{caesar2020nuscenes}  \cite{chang2019argoverse}. However, apart from low resolution, these sensors are difficult to procure and expensive, making everyday personal vehicle adoption challenging.

\textbf{Depth from Adaptive Sensors: } Adaptive sensors use a dynamically controllable light source / receiver instead. These have been making headway in the Long Range Outdoor space. Adaptive Sensing via focal length/baseline variation through the use of servos/motors \cite{Mohamed2018ActiveSP} \cite{4587671} \cite{Nakabo2005VariableBS} \cite{Schneider2018VisuallyGV}, directionally controlled Time-of-Flight Ranging using a MEMS mirror / laser \cite{dtof} \cite{9105183} \cite{pittaluga2020} \cite{8369664}, Gated Depth Imaging \cite{walz2020uncertainty} \cite{gruber2019gated2depth} \cite{10.1117/12.2078169} and finally, sampling specific depth profiles using Triangulation Light Curtains \cite{bartels2019Agile} \cite{wang2018programmable} \cite{Ancha_2020_ECCV} are just some examples. However, these methods do not seem to exploit or fuse data from RGB modalities yet.

\textbf{Depth from RGB: } Depth from Monocular, Stereo or Multi-Camera RGB has been extensively studied and deployed. We focus on a class of Probabilistic Depth estimation approaches that have reformulated the problem as a prediction of per-pixel depth distribution \cite{liu2019neural} \cite{yang2019inferring} \cite{chang2018pyramid} \cite{yao2018mvsnet} \cite{ilg2018uncertainty} \cite{xia2019generating}. Some of this work has actually passively exploited and refined \cite{liu2019neural} \cite{xia2019generating} the uncertainty in the depth values via Moving Cameras and Multi-View-Camera constraints, but have not used the capabilities of the slew of Adaptive Sensors available. 

We hope to fill this gap by investigating if a Probabilistic Depth representation from RGB sensors can be exploited by an Adaptive Sensor such as a Light Curtain to potentially match the precision of LIDARs but in a low cost manner..