% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{stackrel}
\usepackage{float}
\newcommand{\X}{\mathbf{X}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\dtmax}{\Delta \theta_\text{max}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only

\begin{document}

%%%%%%%%% TITLE
\title{ActivFusion: Active Sensing via Exploiting and Refining RGB Depth Distributions using Triangulating Light Curtains}

\author{Yaadhav Raaj ~ Siddharth Ancha ~ Robert Tamburo ~ David Held ~ Srinivasa G. Narasimhan\\
Carnegie Mellon University\\
{\tt\small \{ryaadhav, sancha, rtamburo, dheld, srinivas\}@cs.cmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Active sensing through the use of Adaptive LIDAR's is a nascent field, with major potential in areas such as Advanced driver-assistance systems (ADAS). They do however require electronically or physically driving a laser/light-source to a specific location to capture information, with one such class of sensor being the Programmable Triangulating Light Curtain. In this work, we introduce a novel approach we call (ActivFusion), that exploits prior depth distributions from RGB camera's to drive a Light Curtain's laser such that depth uncertainity and errors get corrected recursively. We also present real-world experiments that validate our approach in outdoor and driving settings.
\end{abstract}

\iffalse
seperately put the lidar pointcloud
monocular depth image
then lidar
then light curtain
teaser

Figure 2 from the Agile Depth Sensing   

mu + sigma - sigma * N

Fig 13 - Spread 1st 2nd and 3rd std

Fig 17 - Integers Numerical

Experiments - Put graph of Train from scratch in (its running)
Yes but this is because of the 0.7 thing?

Train defailt_expy_lc_ilim with prob mode 0.7
Experinment for Stereo Version
Experiment for Train with prob mode 1.0?
Experiment for Reducing the Itertaions or using diff policy

\fi


%%%%%%%%% BODY TEXT
\section{Introduction}

Spinning LIDAR's have been the de-facto sensor of choice in safety critical systems such as Advanced driver-assistance systems (ADAS), due to their reliability in depth estimation. However, their sparse data and cost prohibitive nature has made adoption en-masse rare. Conversely, depth estimation through RGB cameras have been heavily researched, but have not met the same level of reliability. 

We propose to solve the limitation's of RGB only depth estimation through actively sensing areas that are uncertain. One such sensor is the Programmable Triangulating Light Curtain \cite{bartels2019Agile}, which has demonstrated viability in being a low cost (1k\$ vs lidar's~25k\$), high spatial angular resolution (0.02$\degree$ vs lidar's  0.4$\degree$), and high frame-rate (~60fps vs lidar's~20fps) sensor. But as with any active sensor, we must specifiy the region to be sensed via a 3D ruled surface.

We have devised an approach we call \textit{ActivFusion}, which is designed for fusing the modalities of RGB and Active Sensor's such as Light Curtains. We begin by formulating an iterative Bayesian inference approach to depth sensing using the Light Curtain, via Depth Probability Volume (DPV) and it's 2D Uncertainty Field (UF) representations. We then build upon existing planning frameworks based on Dynamic Programming using the sensor's Galvanometer velocity constraints. Lastly, we expand upon Generating and Exploiting Probabilistic Depth Estimates using Deep Learning based approaches, and use that as a prior to drive Light Curtain placement for further refinement.

\input{prior_work}
\input{sensor_setup}
\input{formulation}
\input{lc_only_exp}
\input{network_arch}  

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
