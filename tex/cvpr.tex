% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{stackrel}
\usepackage{float}
\newcommand{\X}{\mathbf{X}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\dtmax}{\Delta \theta_\text{max}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only

\begin{document}

%%%%%%%%% TITLE
\title{Exploiting and Refining Probabilistic Depth Estimates with RGB and Triangulating Light Curtain Fusion}

\author{Yaadhav Raaj ~ Siddharth Ancha ~ Joe Bartels ~ Robert Tamburo ~ Srinivasa G. Narasimhan\\
Carnegie Mellon University\\
{\tt\small \{ryaadhav, sancha, bartels, rtamburo, srinivas\}@cs.cmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Programmable Triangulating Light Curtains are a low-cost, high speed and high resolution approach to depth sensing, but require the user to specify a 2.5D ruled surface over where sensing should occur. Objects that intersect this surface show up with a high intensity, while those that don't do not. We have devised a novel algorithm, that exploits prior depth distributions from either Monocular or Stereo RGB cameras to sample depth in regions of the world most uncertain using the Light Curtain. We then incorporate this data to get a refined depth estimate over time. We evaluate this setup with real world real-time experiments such as in the context of Advanced driver-assistance systems (ADAS)
\end{abstract}

\iffalse
seperately put the lidar pointcloud
monocular depth image
then lidar
then light curtain
teaser

Figure 2 from the Agile Depth Sensing   

mu + sigma - sigma * N

Fig 13 - Spread 1st 2nd and 3rd std

Fig 17 - Integers Numerical

Experiments - Put graph of Train from scratch in (its running)
Yes but this is because of the 0.7 thing?

Train defailt_expy_lc_ilim with prob mode 0.7
Experinment for Stereo Version
Experiment for Train with prob mode 1.0?
Experiment for Reducing the Itertaions or using diff policy

\fi


%%%%%%%%% BODY TEXT
\section{Introduction}

Programmable Triangulating Light Curtains were introduced last year, and have demonstrated viability in being a low cost (1k\$ vs lidar's ~25k\$), high spatial angular resolution (0.02$\degree$ vs lidar's  0.4$\degree$), and high frame-rate (~60fps vs lidar's ~20fps) sensor. They have demonstrated having strong returns in the presence of artifacts such as smoke, and also in seeing small targets such as pedestrians further away. They do however, require a user to specify at what depth per pixel one want's sensed, through providing a physically possible 2.5D ruled surface that intersects the world to provide returns. 

Conversely, depth estimation through RGB cameras have been a heavily researched area, but do not provide the realiability in estimates as compared to a Lidar, which can be cost prohibitive. This has made both RGB and Lidar's en-masse adoption in safety critical systems such as Advanced driver-assistance systems (ADAS) rare. 

Our work, looks into marrying both modalities to counter each sensor's limitations, through the use of Geometric, Probabilistic and Physical constraints. We begin by formulating an iterative Bayesian inference approach to depth sensing using the Light Curtain alone, leveraging on Depth Probability Volume (DPV) and corresponding 2D Uncertainty Field (UF) representations. We then build upon existing planning framework based on Dynamic Programming using the Light Curtain's Galvanometer velocity and acceleration constraints. Lastly, we expand upon Generating and Exploiting Probabilistic Depth Estimates using Deep Learning based approaches, and use that as a prior to drive Light Curtain placement for further refinement.

\input{prior_work}
\input{sensor_setup}
\input{formulation}
\input{lc_only_exp}
\input{network_arch}  

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
