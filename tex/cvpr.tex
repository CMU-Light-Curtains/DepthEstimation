% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{stackrel}
\newcommand{\X}{\mathbf{X}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\dtmax}{\Delta \theta_\text{max}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only

\begin{document}

%%%%%%%%% TITLE
\title{Exploiting and Refining Probabilistic Depth Estimates with RGB and Triangulating Light Curtain Fusion}

\author{Yaadhav Raaj ~ Siddharth Ancha ~ Joe Bartels ~ Robert Tamburo ~ Srinivasa G. Narasimhan\\
Carnegie Mellon University\\
{\tt\small \{ryaadhav, sancha, bartels, rtamburo, srinivas\}@cs.cmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Programmable Triangulating Light Curtains are a low-cost, high speed and high resolution approach to depth sensing, but require the user to specify a 2.5D ruled surface over where sensing should occur. Objects that intersect this surface show up with a high intensity, while those that don't do not. We have devised a novel algorithm, that exploits prior depth distributions from either Monocular or Stereo RGB cameras to sample depth in regions of the world most uncertain using the Light Curtain. We then incorporate this data to get a refined depth estimate over time. We evaluate this setup with real world real-time experiments such as in the context of Advanced driver-assistance systems (ADAS)
\end{abstract}

\iffalse
Introduction
Prior Work
   Joe's Paper
   Sid's Paper
   other stuff
Sensor setup
   Light Curtain Basics
   Our Jeep setup
   Calibration process (eqs. here)
Adaptive Sensing Formulation (say Light Curtain Only)
   Simulator
   3D Tensor to store Depth (initialization)
   Ray 2D Projection into Flat Plane
   Planning Formulation/Strategies Default/M1
   Probabilistic Formulation (Inverted Gaussian Model)
   Experiments (show Sim vs Real world, Indoor vs Outdoor):
      Basic Sweep visualization (sanity check)
      Prior sweeping approach
      Non Inverted vs Inverted
      STD encoding the uncertaintiy etc.
      Default vs M1 (and diff strategies)
   Having a Prior Depth (what if we had)
   Experiments here
         Show convergence speed/accuracy etc.
Generating Prior from RGB
   NN Formulation figure
   Soft CE, QPower, Left/Right Consistency (RGB + Depth), Smoothness
   Experiments:
      Show table for above
   LC Integration
   Experiments:
      Effect/Speedup of having a Prior
      
      3D Upsampling of Lidar
      Time feedback vs no
      Deep Learning vs no Deep Learning (Ruled Surface etc.)
Outdoor Experiments:
   etc.
Future Work
\fi


%%%%%%%%% BODY TEXT
\section{Introduction}

Programmable Triangulating Light Curtains were introduced last year, and have demonstrated viability in being a low cost (1k\$ vs lidar's ~25k\$), high spatial angular resolution (0.02$\degree$ vs lidar's  0.4$\degree$), and high frame-rate (~60fps vs lidar's ~20fps) sensor. They have demonstrated having strong returns in the presence of artifacts such as smoke, and also in seeing small targets such as pedestrians further away. They do however, require a user to specify at what depth per pixel one want's sensed, through providing a physically possible 2.5D ruled surface that intersects the world to provide returns. 

Conversely, depth estimation through RGB cameras have been a heavily researched area, but do not provide the realiability in estimates as compared to a Lidar, which can be cost prohibitive. This has made both RGB and Lidar's en-masse adoption in safety critical systems such as Advanced driver-assistance systems (ADAS) rare. 

Our work, looks into marrying both modalities to counter each sensor's limitations, through the use of Geometric, Probabilistic and Physical constraints. We begin by formulating an iterative Bayesian inference approach to depth sensing using the Light Curtain alone, leveraging on Depth Probability Volume (DPV) and corresponding 2D Uncertainty Field (UF) representations. We then build upon existing planning framework based on Dynamic Programming using the Light Curtain's Galvanometer velocity and acceleration constraints. Lastly, we expand upon Generating and Exploiting Probabilistic Depth Estimates using Deep Learning based approaches, and use that as a prior to drive Light Curtain placement for further refinement.

\input{prior_work}
\input{sensor_setup}
\input{formulation}
\input{lc_only_exp}
\input{network_arch}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
