% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{stackrel}
\usepackage{float}
\newcommand{\X}{\mathbf{X}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\dtmax}{\Delta \theta_\text{max}}
\newcommand{\sid}[1]{{\color{red} \textbf{Sid:} #1}}
\usepackage{caption}
\captionsetup{font=footnotesize}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only

\begin{document}

%%%%%%%%% TITLE
\title{Exploiting \& Refining Depth Distributions with Triangulation Light Curtains}

\author{Yaadhav Raaj ~ Siddharth Ancha ~ Robert Tamburo ~ David Held ~ Srinivasa G. Narasimhan\\
Carnegie Mellon University\\
{\tt\small \{ryaadhav, sancha, rtamburo, dheld, srinivas\}@cs.cmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}


\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Active sensing through the use of Adaptive Depth Sensors is a nascent field, with major potential in areas such as Advanced driver-assistance systems (ADAS). They do however require electronically or physically driving a laser/light-source to a specific location to capture information, with one such class of sensor being the Triangulation Light Curtain. In this work, we introduce a novel approach  that exploits prior depth distributions from RGB cameras to drive a Light Curtain's laser such that depth uncertainty and errors get corrected and refined recursively. We also present real-world experiments that validate our approach in outdoor and driving settings.
\end{abstract}

%%%%%%%%% BODY TEXT
\vspace{-.1in}
\section{Introduction}

Spinning Fixed Scan LIDARs have been the de-facto sensor of choice in safety critical systems such as Advanced driver-assistance systems (ADAS), due to their reliability in depth estimation. However, their reduced spatial-resolution, multi-path interference and their cost prohibitive nature has made en-masse adoption in personal vehicles rare. To counter these issues, depth estimation through RGB cameras has been heavily researched. However, issues such as oversaturation and feature correspondence errors in specular objects, scale ambiguity and unreliability in contrast with LIDARs has made relying on these sensors unsafe.

To capture the error and uncertainty in RGB-only depth estimation, previous work had formulated that task as a probabilistic regression problem, by predicting per-pixel depth distributions via a Depth Probability Volume (DPV) \cite{liu2019neural} \cite{chang2018pyramid} \cite{yang2019inferring}. It provides both a Maximum Likelihood-Estimate (MLE) of the depth map, as well as the corresponding per-pixel uncertainty measure. However, these works do not adaptively or physically correct for this uncertainty, instead relying purely on Multi-View-Camera constraints for passive correction.

In this work, we have devised the first known framework that \textit{adaptively} exploits the depth uncertainty in a DPV (generated from RGB cameras) and refines it via a class of Adaptive Depth Sensors called a Triangulating Light Curtain \cite{bartels2019Agile}. It has a steerable Laser Line that can be driven in tandem with a Rolling Shutter camera to generate a 3D ruled surface to sample the world. We have chosen this sensor due to its low cost (\$1k vs lidar~$\sim$\$25k), high spatial angular resolution (0.02$\degree$ vs lidar 0.4$\degree$), and high frame-rate (~60fps vs lidar 20fps).

We begin by formulating an iterative Bayesian inference approach to depth sensing using the Light Curtain only, build upon planning and sensing frameworks based on Dynamic Programming using the it's Galvomirror constraints and lastly, expand upon generating and exploiting probabilistic depth estimates using Deep Learning based approaches combining both modalities of RGB and Light Curtain. We perform extensive experiments both in Simulation (using KITTI \cite{Geiger2013IJRR} dataset) and in the real-world using our sensor setup, and demonstrate better depth estimation results when both modalities are combined.

\input{prior_work}
\input{sensor_setup}
\input{formulation}
\input{lc_only_exp}
\input{network_arch}  

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
